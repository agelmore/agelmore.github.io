---
layout: post
title:  "Megahit"
date:   2014-11-16
comments: true
---

[Megahit](https://github.com/voutcn/megahit) is a relatively new NGS assembler that can *de novo* assemble very large data sets from metagenomic samples. I used it to assemble the [synthetic metadata](https://export.uppmax.uu.se/b2010008/projects-public/concoct-paper-data/) that I've been playing with from the CONCOCT paper published in *Nature Methods* by [Alneburg et al.](http://www-ncbi-nlm-nih-gov.proxy.lib.umich.edu/pubmed/?term=binning+metagenomic+contigs+by+coverage+and+composition)

Titas Brown says that megahit is [pretty good](http://ivory.idyll.org/blog/2014-how-good-is-megahit.html) and it is faster and more memory efficient. Digital normalization is not needed. I cloned megahit into /mnt/EXT/Schloss-data/amanda/Fuso/megahit/megahit. It's pretty simple to use. It will take fasta, fastq, or zipped files. I don't think it can take paired read information, but it will cat my two paired files so I don't have to interleave them.

```
python ./megahit -m 45e9 --input-cmd 'cat /mnt/EXT/Schloss-data/amanda/Fuso/concoct/testdata/species/run1/All_*' --cpu-only -l 100 -o /mnt/EXT/Schloss-data/amanda/Fuso/concoct/testdata/species/run1/megahit
```

**Side note:** I submit all my jobs using Kathryn's sweet quicksubmit command. I set a global variable in my bash_profile that makes standard parameters

```
export quickpara="--pm nodes=1:ppn=8,mem=48GB --walltime 999:00:00 --cput 999:00:00"
```

so I just have to submit the following:

```
quicksubmit "command" --jobname [jobname] $quickpara
```

Okay so back to the assembly. It went pretty fast, but after closer examination it didn't do a complete assembly. The program stopped reading in the file at 1205357142 reads because it ran out of memory. There are reads total in the file so it dropped about %. But it still assembled those reads, taking about 50 hours. Here is the logfile:

{% gist f11c92c39fb26187ea56 %}

Alright, so some assembly statistics. I used some awk scripts to calculate average sequence length and number greater than 500bp and 1kb.

{% highlight bash %}

awk '{/>/&&++a||b+=length()}END{print b/a}' final.contigs.fa #find average sequence length
grep -c '>' final.contigs.fa #counts number of contigs

awk '!/^>/ {next} {getline s} length(s) >= 500 { print $0 "\n" s }' final.contigs.fa > final.contigs.500.fa #pulls out all the contigs greater than 500bp

grep -c '>' final.contigs.500.fa

awk '!/^>/ {next} {getline s} length(s) >= 1000 { print $0 "\n" s }' final.contigs.fa > final.contigs.1000.fa
grep -c '>' final.contigs.1000.fa

{% endhighlight %}

|Average sequence length| 3650.39 bp|
|---|---|
|Number of total contigs| 100419|
|Number contigs > 500bp| 19663|
|Number contigs > 1kb| 12741|


There are 12741 contigs greater than 1kb. In the CONCOCT paper they processed the contigs to filter out any less than 1kb because the composition vectors with the frequencies of each kmer isn't accurate when contigs are shorter than 1kb. This may throw away some of the rare bugs that couldn't assemble to greater than 1kb, but it will also get rid of errors from too short contigs. 

Next post I'll try some other assemblers and then I'll try running those through the CONCOCT pipeline.

<*insert witty joke like Kathryn would have*>

