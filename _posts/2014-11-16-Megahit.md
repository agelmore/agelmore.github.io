---
layout: post
title:  "Megahit"
date:   2014-11-16
comments: true
---

[Megahit](https://github.com/voutcn/megahit) is a relatively new NGS assembler that can *de novo* assemble very large data sets from metagenomic samples. I used it to assemble the [synthetic metadata](https://export.uppmax.uu.se/b2010008/projects-public/concoct-paper-data/) that I've been playing with from the CONCOCT paper published in *Nature Methods* by [Alneburg et al.](http://www-ncbi-nlm-nih-gov.proxy.lib.umich.edu/pubmed/?term=binning+metagenomic+contigs+by+coverage+and+composition)

Titas Brown says that megahit is [pretty good](http://ivory.idyll.org/blog/2014-how-good-is-megahit.html) and it is faster and more memory efficient. Digital normalization is not needed. I cloned megahit into /mnt/EXT/Schloss-data/amanda/Fuso/megahit/megahit. It's pretty simple to use. It will take fasta, fastq, or zipped files. I don't think it can take paired read information.

```
python ./megahit -m 45e9 --input-cmd 'cat /mnt/EXT/Schloss-data/amanda/Fuso/concoct/testdata/species/run1/All_*' --cpu-only -l 100 -o /mnt/EXT/Schloss-data/amanda/Fuso/concoct/testdata/species/run1/megahit
```

**Side note:** I submit all my jobs using Kathryn's sweet quicksubmit command. I set a global variable in my bash_profile that makes standard parameters

```
export quickpara="--pm nodes=1:ppn=8,mem=48GB --walltime 999:00:00 --cput 999:00:00"
```

so I just have to submit the following:

```
quicksubmit "command" --jobname [jobname] $quickpara
```

Okay so back to the assembly. It went pretty fast, but after closer examination it didn't do a complete assembly. The program stopped reading in the file at 1205357142 reads because it ran out of memory. But it still assembled those reads, taking about 50 hours. Here is the logfile:

```
[Sun Oct 19 16:47:39 2014]: Extracting solid (k+1)-mers for k = 21
[WARNING] the number of reads is too large, only keep the first 1205357142 ones.
 The assembly will be incomplete.
[Mon Oct 20 22:01:08 2014]: Building graph for k = 21
[Mon Oct 20 22:14:46 2014]: Assembling contigs from SdBG for k = 21
[Mon Oct 20 23:08:36 2014]: Extracting iterative edges from k = 21 to 31
[Tue Oct 21 05:09:51 2014]: Building graph for k = 31
[Tue Oct 21 05:32:18 2014]: Assembling contigs from SdBG for k = 31
[Tue Oct 21 06:09:50 2014]: Extracting iterative edges from k = 31 to 41
[Tue Oct 21 08:10:57 2014]: Building graph for k = 41
[Tue Oct 21 08:41:57 2014]: Assembling contigs from SdBG for k = 41
[Tue Oct 21 09:20:53 2014]: Extracting iterative edges from k = 41 to 51
[Tue Oct 21 10:58:00 2014]: Building graph for k = 51
[Tue Oct 21 11:31:56 2014]: Assembling contigs from SdBG for k = 51
[Tue Oct 21 12:08:37 2014]: Extracting iterative edges from k = 51 to 61
[Tue Oct 21 13:23:38 2014]: Building graph for k = 61
[Tue Oct 21 14:04:54 2014]: Assembling contigs from SdBG for k = 61
[Tue Oct 21 14:37:29 2014]: Extracting iterative edges from k = 61 to 71
[Tue Oct 21 15:34:24 2014]: Building graph for k = 71
[Tue Oct 21 16:10:04 2014]: Assembling contigs from SdBG for k = 71
[Tue Oct 21 16:34:58 2014]: Extracting iterative edges from k = 71 to 81
[Tue Oct 21 17:07:34 2014]: Building graph for k = 81
[Tue Oct 21 17:32:14 2014]: Assembling contigs from SdBG for k = 81
[Tue Oct 21 17:47:43 2014]: Extracting iterative edges from k = 81 to 91
[Tue Oct 21 18:16:39 2014]: Building graph for k = 91
[Tue Oct 21 18:33:52 2014]: Assembling contigs from SdBG for k = 91
[Tue Oct 21 18:41:49 2014]: Extracting iterative edges from k = 91 to 99
[Tue Oct 21 18:49:16 2014]: Building graph for k = 99
[Tue Oct 21 18:57:35 2014]: Assembling contigs from SdBG for k = 99
[Tue Oct 21 19:01:14 2014]: Merging to output final contigs.
[Tue Oct 21 19:01:22 2014]: ALL DONE.
Number of CPU threads 16
```

awk '{/>/&&++a||b+=length()}END{print b/a}' final.contigs.fa #find average sequence length
3650.39 bp
grep -c '>' final.contigs.fa #counts number of contigs
100419

awk '!/^>/ {next} {getline s} length(s) >= 500 { print $0 "\n" s }' final.contigs.fa > final.contigs.500.fa #pulls out all the contigs greater than 500bp

grep -c '>' final.contigs.500.fa
19663 contigs greater than 500bp

awk '!/^>/ {next} {getline s} length(s) >= 1000 { print $0 "\n" s }' final.contigs.fa > final.contigs.1000.fa
grep -c '>' final.contigs.1000.fa
12741 contigs greater than 1kb
```

There are 12741 contigs greater than 1kb. In the CONCOCT paper they processed the contigs to filter out any less than 1kb because the composition vectors with the frequencies of each kmer isn't accurate when contigs are shorter than 1kb. This may throw away some of the rare bugs that couldn't assemble to greater than 1kb, but it will also get rid of errors from too short contigs. 

